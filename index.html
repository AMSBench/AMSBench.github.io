<!DOCTYPE html>
<html>
<head>
Â  <meta charset="utf-8">
Â  Â  Â  <meta name="description" content="AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits.">
Â  <meta property="og:title" content="AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits"/>
Â  <meta property="og:description" content="We introduce AMSbench, a comprehensive benchmark to evaluate Multimodal Large Language Models (MLLMs) in the domain of Analog/Mixed-Signal (AMS) circuits, covering perception, analysis, and design."/>
Â  <meta property="og:url" content="URL_OF_THE_WEBSITE"/>
Â  Â  <meta property="og:image" content="static/image/your_banner_image.png" />
Â  <meta property="og:image:width" content="1200"/>
Â  <meta property="og:image:height" content="630"/>


Â  <meta name="twitter:title" content="AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits">
Â  <meta name="twitter:description" content="We introduce AMSbench, a comprehensive benchmark to evaluate MLLMs in the domain of AMS circuits, covering perception, analysis, and design.">
Â  Â  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
Â  <meta name="twitter:card" content="summary_large_image">
Â  Â  <meta name="keywords" content="MLLM, Benchmark, Analog Circuits, Mixed-Signal, EDA, Deep Learning">
Â  <meta name="viewport" content="width=device-width, initial-scale=1">


Â  <title>AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits</title>
Â  <link rel="icon" type="image/x-icon" href="static/images/logo.png">
Â  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
Â  rel="stylesheet">

Â  <link rel="stylesheet" href="static/css/bulma.min.css">
Â  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
Â  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
Â  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
Â  <link rel="stylesheet"
Â  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
Â  <link rel="stylesheet" href="static/css/index.css">

<style>
    .compact-section {
        padding-top: 2.5rem;
        padding-bottom: 2.5rem;
    }
    .results-image {
        max-width: 100%;
        max-height: 400px;
        margin: auto;
        border-radius: 8px;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .carousel-image {
        max-height: 480px;
        width: auto;
        object-fit: contain;
        margin: auto;
    }
    .hero.teaser .hero-body {
        padding-top: 2rem;
        padding-bottom: 2rem;
    }
    .publication-title {
        margin-bottom: 1.5rem !important;
    }
</style>
Â  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
Â  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
Â  <script defer src="static/js/fontawesome.all.min.js"></script>
Â  <script src="static/js/bulma-carousel.min.js"></script>
Â  <script src="static/js/bulma-slider.min.js"></script>
Â  <script src="static/js/index.js"></script>
</head>
<body>


    Â    <section class="hero">
        <div class="hero-body">
          <div class="container is-max-desktop">
            <div class="columns is-centered">
              <div class="column has-text-centered">
                <h1 class="title is-1 publication-title">AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits</h1>
    
                <div class="is-size-5 publication-authors">
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=m5NipB4AAAAJ&hl=en" target="_blank">Yichen Shi</a><sup>1,4,*</sup>,
                  </span>
                  <span class="author-block">
                    <a href="AUTHOR_LINK_HERE" target="_blank">Ze Zhang</a><sup>4,*</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=UDzaFPIAAAAJ&hl=en" target="_blank">Hongyang Wang</a><sup>4,*</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=MszI2TAAAAAJ&hl=zh-CN&oi=ao" target="_blank">Zhuofu Tao</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="AUTHOR_LINK_HERE" target="_blank">Zhongyi Li</a><sup>1</sup>,
                  </span>
                  <span class="author-block">
                    <a href="AUTHOR_LINK_HERE" target="_blank">Bingyu Chen</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="AUTHOR_LINK_HERE" target="_blank">Yaxin Wang</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=wU8x220AAAAJ&hl=en" target="_blank">Zhiping Yu</a><sup>3</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=P8suglMAAAAJ&hl=zh-CN&oi=ao" target="_blank">Ting-Jung Lin</a><sup>4,**</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=n_N-PJkAAAAJ&hl=zh-CN" target="_blank">Lei He</a><sup>1,2,4,**</sup>
                  </span>
                </div>
      
                <div class="is-size-5 publication-authors">
                  <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University</span>
                  <span class="author-block"><sup>2</sup>University of California, Los Angeles</span>
                  <span class="author-block"><sup>3</sup>Tsinghua University</span>
                  <span class="author-block"><sup>4</sup>Eastern Institute of Technology, Ningbo</span>
                  <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  <span class="eql-cntrb"><small><br><sup>**</sup>Indicates Corresponding authors</small></span>
                </div>
Â  
Â  	  				<div class="column has-text-centered">
Â  	  				  <div class="publication-links">
Â  	  						  Â  	  						<span class="link-block">
Â  	  						  <a href="https://arxiv.org/abs/2505.24138.pdf" target="_blank"
Â  	  						  class="external-link button is-normal is-rounded is-dark">
Â  	  						  <span class="icon">
Â  	  							  <i class="fas fa-file-pdf"></i>
Â  	  						  </span>
Â  	  						  <span>Paper</span>
Â  	  						</a>
Â  	  					  </span>
Â  	  					  
Â  	  					  Â  	  					  <span class="link-block">
Â  	  						<a href="https://huggingface.co/datasets/wwhhyy/AMSBench" target="_blank"
Â  	  						class="external-link button is-normal is-rounded is-dark">
Â  	  						<span class="icon" style="font-size:18px">ðŸ¤—</span>
Â  	  						<span>Dataset</span>
Â  	  					  </a>
Â  	  					</span>
Â  	  					
Â  
Â  	  					  Â  	  					  <span class="link-block">
Â  	  						<a href="static/pdfs/2505.24138v1.pdf" target="_blank"
Â  	  						class="external-link button is-normal is-rounded is-dark">
Â  	  						<span class="icon">
Â  	  							<i class="fas fa-file-pdf"></i>
Â  	  						</span>
Â  	  						<span>Supplementary</span>
Â  	  					  </a>
Â  	  					</span>
Â  
Â  	  				  Â  	  				  <span class="link-block">
Â  	  					<a href="https://github.com/Why0912/AMSBench" target="_blank"
Â  	  					class="external-link button is-normal is-rounded is-dark">
Â  	  					<span class="icon">
Â  	  						<i class="fab fa-github"></i>
Â  	  					</span>
Â  	  					<span>Code</span>
Â  	  				  </a>
Â  	  				</span>
Â  
Â  	  			  Â  	  			  <span class="link-block">
Â  	  				<a href="https://arxiv.org/abs/2505.24138v1" target="_blank"
Â  	  				class="external-link button is-normal is-rounded is-dark">
Â  	  				<span class="icon">
Â  	  					<i class="ai ai-arxiv"></i>
Â  	  				</span>
Â  	  				<span>arXiv</span>
Â  	  			  </a>
Â  	  			</span>
Â  	  			</div>
Â  	  		  </div>
Â  	  		</div>
Â  	  	  </div>
Â  	  	</div>
Â  	  </div>
Â  </section>
Â  
Â  
<section class="hero teaser">
Â  <div class="hero-body" style="padding-top: 0; padding-bottom: 2rem;">
Â  	<figure class="image is-fullwidth">
Â  	  <img src="static/images/AMSBench_banner.png" alt="AMSbench Teaser"
Â  			 style="width: 60%; height: auto; display: block; margin: 0 auto;">
Â  	</figure>
Â  	<h2 class="subtitle has-text-centered mt-4">
Â  	  AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits
Â  	</h2>
Â  </div>
</section>
Â  
<section class="section compact-section">
Â  <div class="container is-max-desktop">
Â  	<h2 class="title is-3">Abstract</h2>
Â  	<div class="content has-text-justified">
Â  	  <p>
Â  		Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated circuit (IC) industry. 
Â  		However, automating AMS circuit design has remained a longstanding challenge due to its difficulty and complexity. 
Â  		Recent advances in Multi-modal Large Language Models (MLLMs) offer promising potential for supporting AMS circuit analysis and design. 
Â  		However, current research typically evaluates MLLMs on isolated tasks within the domain, 
Â  		lacking a comprehensive benchmark that systematically assesses model capabilities across diverse AMS-related challenges.
Â  	  </p>
Â  	  <p>
Â  		To address this gap, we introduce <strong>AMSbench</strong>, a benchmark suite designed to evaluate MLLM performance across critical tasks 
Â  		including circuit schematic perception, circuit analysis, and circuit design. 
Â  		AMSbench comprises approximately 8000 test questions spanning multiple difficulty levels and assesses eight prominent models, 
Â  		encompassing both open-source and proprietary solutions such as Qwen 2.5-VL and Gemini 2.5 Pro.
Â  	  </p>
Â  	  <p>
Â  		Our evaluation highlights significant limitations in current MLLMs, particularly in complex multi-modal reasoning and sophisticated circuit design tasks. 
Â  		These results underscore the necessity of advancing MLLMsâ€™ understanding and effective application of circuit-specific knowledge, 
Â  		thereby narrowing the existing performance gap relative to human expertise and moving toward fully automated AMS circuit design workflows.
Â  		Our data is released at 
Â  		<a href="https://huggingface.co/datasets/wwhhyy/AMSBench" target="_blank">this URL</a>.
Â  	  </p>
Â  	</div>
Â  </div>
</section>


<section class="hero is-light">
Â  <div class="hero-body compact-section has-text-centered">
Â  	<h1 class="title is-2 mmmu">
Â  	  <span class="mmmu">The AMSBench Benchmark</span>
Â  	</h1>
Â  </div>
</section>
Â  
<section class="section compact-section">
    <div class="container is-max-desktop">
        <div class="content has-text-justified" style="margin-bottom: 3rem;">
            <h2 class="title is-3 has-text-centered">Benchmark Introduction</h2>
            <p>
                The design of Analog/Mixed-Signal (AMS) circuits is highly dependent on human expertise, and its automation has been a long-standing challenge. Although Multimodal Large Language Models (MLLMs) have achieved breakthroughs in many fields, their application in the AMS domain remains limited, and a comprehensive evaluation framework to systematically measure their capabilities is lacking. To fill this gap, we propose <strong>AMSbench</strong>, the first comprehensive benchmark designed to rigorously evaluate the three core capabilities of MLLMs in the AMS domain: <strong>Perception, Analysis, and Design</strong>.
            </p>
        </div>

        <div class="columns is-vcentered">
            <div class="column">
                <h3 class="title is-4">Data Collection & Curation</h3>
                <p class="has-text-justified">
                    To build a comprehensive benchmark, we collected data from various sources, including academic textbooks, research papers, and industrial datasheets. We used tools like MinerU to convert PDFs and AMSnet to generate netlists from schematics. We then combined expert annotations with MLLM outputs to create high-quality "circuit-caption" data pairs.
                </p>
            </div>
            <div class="column">
                <figure class="image">
                    <img src="static/images/data_collection.png" alt="Data Collection Pipeline" class="results-image">
                </figure>
            </div>
        </div>

        <div class="columns is-vcentered" style="margin-top: 2rem;">
            <div class="column">
                <figure class="image">
                    <img src="static/images/question_generation.png" alt="Question Generation Examples" class="results-image">
                </figure>
            </div>
            <div class="column">
                <h3 class="title is-4">Question Generation & Task Design</h3>
                <p class="has-text-justified">
                    AMSbench covers both Visual and Textual Question Answering (VQA/TQA) with multiple formats. Questions are tiered into three difficulty levels (Easy, Medium, Hard) to simulate knowledge requirements from undergraduate to professional engineer levels, ensuring a thorough evaluation of model capabilities.
                </p>
            </div>
        </div>
    </div>
</section>

<section class="section compact-section">
    <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">Evaluation & Findings</h2>
        <div class="columns is-vcentered">
            <div class="column is-6">
                <p class="has-text-justified">
                    We evaluated 8 leading models, including GPT-4o and Gemini-2.5-pro. Our findings reveal significant limitations in current SOTA models, especially in complex reasoning and design.
                </p>
                <ul>
                    <li><b>Perception:</b> Models struggle to extract complete, accurate netlists.</li>
                    <li><b>Analysis:</b> They show potential but fail to grasp key performance trade-offs.</li>
                    <li><b>Design:</b> Performance is poor on complex circuits, and models cannot generate valid testbenches.</li>
                </ul>
            </div>
            <div class="column is-6">
                 <figure class="image">
                    <img src="static/images/rada.png" alt="Model Performance Radar Chart" class="results-image">
                 </figure>
            </div>
        </div>
        <div class="columns" style="margin-top: 2rem;">
            <div class="column"><img src="static/images/tab_perception.png" alt="Perception Task Results" class="results-image"></div>
            <div class="column"><img src="static/images/tab_multitask.png" alt="Interconnect and Analysis Results" class="results-image"></div>
        </div>
        <div class="has-text-centered" style="margin-top: 1rem;">
            <figure class="image" style="max-width: 600px; margin: auto;">
                 <img src="static/images/tab_design_tb.png" alt="Design Task Results" class="results-image">
            </figure>
        </div>
    </div>
</section>

<section class="section compact-section">
    <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">Data Statistics</h2>
        <p class="has-text-centered">The benchmark is carefully balanced across tasks and difficulty levels to provide a robust evaluation framework.</p>
        <figure class="image">
            <img src="static/images/data.png" alt="Data Statistics Pie Charts" style="max-width: 900px; margin: auto;">
        </figure>
    </div>
</section>
<section class="hero is-light">
    <div class="hero-body compact-section has-text-centered">
        <h1 class="title is-2 mmmu">
            <span class="mmmu">Task Examples</span>
        </h1>
    </div>
</section>

<section class="section compact-section">
Â  <div class="container is-max-desktop">
Â  	  <div id="results-carousel" class="carousel results-carousel">
Â  	  	<div class="item has-text-centered">
Â  	  	  <img src="static/images/perception.png" alt="Perception Task Example" class="carousel-image" />
Â  	  	  <h2 class="subtitle has-text-centered mt-4">
Â  	  	  	<b>Perception Task:</b> Includes total/type-wise counting, location description, connection judgment, and topology generation.
Â  	  	  </h2>
Â  	  	</div>
Â  	  	<div class="item has-text-centered">
Â  	  	  <img src="static/images/partition.png" alt="Partition Task Example" class="carousel-image" />
Â  	  	  <h2 class="subtitle has-text-centered mt-4">
Â  	  	  	<b>Partition Task:</b> The model must identify all constituent structures in a complex circuit, such as differential pair and Miller compensation.
Â  	  	  </h2>
Â  	  	</div>
Â  	  	<div class="item has-text-centered">
Â  	  	  <img src="static/images/reasoning.png" alt="Reasoning Task Example" class="carousel-image" />
Â  	  	  <h2 class="subtitle has-text-centered mt-4">
Â  			   <b>Reasoning Task:</b> Based on the circuit diagram, the model must explain why it functions as an operational amplifier.
Â  	  	  </h2>
Â  	  	</div>
Â  	  	<div class="item has-text-centered">
Â  	  	  <img src="static/images/function.png" alt="Function Task Example" class="carousel-image" />
Â  	  	  <h2 class="subtitle has-text-centered mt-4">
Â  			   <b>Function Task:</b> The model must determine the intended function of the given circuit diagram.
Â  	  	  </h2>
Â  	  	</div>
Â  	  </div>
  </div>
</section>
<section class="hero is-light compact-section">
Â  <div class="hero-body">
Â  	<div class="container">
Â  	  <h2 class="title has-text-centered">Poster</h2>
      Â  	  <div id="adobe-pdf-view" style="height: 700px; border: 1px solid #dbdbdb;"></div>
Â  	</div>
Â  </div>
</section>
Â  <section class="section compact-section" id="BibTeX">
Â  	<div class="container is-max-desktop content">
Â  	  <h2 class="title">BibTeX</h2>
Â  	  <pre><code>@misc{shi2025amsbenchcomprehensivebenchmarkevaluating,
Â  			title={AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits}, 
Â  			author={Yichen Shi and Ze Zhang and Hongyang Wang and Zhuofu Tao and Zhongyi Li and Bingyu Chen and Yaxin Wang and Zhiping Yu and Ting-Jung Lin and Lei He},
Â  			year={2025},
Â  			eprint={2505.24138},
Â  			archivePrefix={arXiv},
Â  			primaryClass={cs.LG},
Â  			url={https://arxiv.org/abs/2505.24138}, 
Â  }</code></pre>
Â  	</div>
</section>
Â  <footer class="footer">
Â  <div class="container">
Â  	<div class="columns is-centered">
Â  	  <div class="column is-8">
Â  		<div class="content">
Â  
Â  		  <p>
Â  			This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
Â  			You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" Â href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
Â  			Commons Attribution-ShareAlike 4.0 International License</a>.
Â  		  </p>
Â  
Â  		</div>
Â  	  </div>
Â  	</div>
Â  </div>
</footer>

<script type="text/javascript">
    document.addEventListener("adobe_dc_view_sdk.ready", function () {
        // IMPORTANT: To make this work on your website, you need a client ID.
        // 1. Get a free client ID: https://developer.adobe.com/document-services/apis/pdf-embed/
        // 2. Replace "<YOUR_ADOBE_CLIENT_ID>" below with your actual ID.
        var adobeDCView = new AdobeDC.View({
            clientId: "<YOUR_ADOBE_CLIENT_ID>", // <--- REPLACE THIS
            divId: "adobe-pdf-view"
        });
        adobeDCView.previewFile({
            content: {
                // Ensure the PDF file is located at this path
                location: { url: "static/pdfs/2505.24138v1.pdf" }
            },
            metaData: {
                fileName: "AMSbench_Poster.pdf"
            }
        }, {
            embedMode: "SIZED_CONTAINER",
            showPrintPDF: true,
            showDownloadPDF: true
        });
    });
</script>
Â  </body>
Â  </html>